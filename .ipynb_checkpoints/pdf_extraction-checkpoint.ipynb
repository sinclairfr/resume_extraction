{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f53fedf-1dc4-4bc7-8c31-930b2093770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/medo/resume_extraction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c4ae762-ba8a-42e3-90ca-2c8eff874b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_path = \"BSV_VITI_LR_N01_22032022.pdf\"\n",
    "pdf_path = \"CV Mehdi Fekih.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68dfc7-504b-4469-9034-cf1ad23f800b",
   "metadata": {},
   "source": [
    "# PDF Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8aeb8c1c-6fbd-44bf-812d-467f8d0ae2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6b49f52-bcf3-42df-98a4-7b8cb295441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages, extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c07eaa2-588d-4046-9293-db897c69e151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "<LTTextBoxHorizontal(0) 237.824,801.721,358.204,823.721 'Mehdi Fekih\\n'>\n",
      "<LTTextBoxHorizontal(1) 204.424,759.911,391.608,785.161 '8 rue Bernard de Clairvaux, 75003 Paris\\n07 82 90 60 71 – mehdi.fekih@edhec.com\\n'>\n",
      "<LTTextBoxHorizontal(2) 28.800,730.234,200.169,742.234 '■ FORMATION ET DIPLÔMES\\n'>\n",
      "<LTTextBoxHorizontal(3) 40.800,704.087,60.794,714.087 '2022\\n'>\n",
      "<LTTextBoxHorizontal(4) 139.800,704.087,335.530,714.087 'Data Scientist, Titre RNCP de niveau 7 (Bac+5)\\n'>\n",
      "<LTTextBoxHorizontal(5) 40.800,680.087,90.782,690.087 '2005 – 2009\\n'>\n",
      "<LTTextBoxHorizontal(6) 139.800,668.087,502.511,690.087 'EDHEC BUSINESS SCHOOL, Master in Management – Campus de Lille\\nSpécialisation entrepreneuriat. English Track Program (scolarité entièrement en anglais).\\n'>\n",
      "<LTTextBoxHorizontal(7) 139.800,644.087,462.073,654.087 'Université Paris 7, Licence de Mathématiques Appliquées et Sciences Sociales.\\n'>\n",
      "<LTTextBoxHorizontal(8) 40.800,620.087,90.782,630.087 '2002 – 2004\\n'>\n",
      "<LTTextBoxHorizontal(9) 139.800,620.087,445.917,630.087 'Classe préparatoire aux Grandes Écoles de Commerce, voie scientifique.\\n'>\n",
      "<LTTextBoxHorizontal(10) 40.800,596.087,60.794,606.087 '2002\\n'>\n",
      "<LTTextBoxHorizontal(11) 40.800,572.087,76.910,582.087 'Langues\\n'>\n",
      "<LTTextBoxHorizontal(12) 139.800,596.087,470.722,606.087 'Baccalauréat Scientifique (spécialisation Mathématiques), mention AB – Chelles\\n'>\n",
      "<LTTextBoxHorizontal(13) 139.800,536.087,406.587,582.087 'Anglais : Courant (C1) -TOEFL IBT 111/120, TOEIC 980/1000\\nEspagnol : Niveau moyen (B2)\\nItalien : Notions (A2)\\nThaïlandais : Notions (A1)\\n'>\n",
      "<LTTextBoxHorizontal(14) 28.800,509.734,251.439,521.734 '■ EXPÉRIENCES PROFESSIONNELLES\\n'>\n",
      "<LTTextBoxHorizontal(15) 40.800,483.587,60.794,493.587 '2023\\n'>\n",
      "<LTTextBoxHorizontal(16) 139.800,483.587,511.451,493.587 'Co-fondateur - Data Scientist et Data Analyst, PME et industrie, Prismanalytics – Paris\\n'>\n",
      "<LTTextBoxHorizontal(17) 154.800,423.587,516.734,469.631 '● Analyse de données et performance des entreprises.\\n● Modélisation statistique et machine learning.\\n● Visualisation de données et tableaux de bord interactifs.\\n● Gestion de projet et développement de solutions personnalisées en Machine Learning.\\n'>\n",
      "<LTTextBoxHorizontal(18) 40.800,399.587,84.115,409.587 '2019-2021\\n'>\n",
      "<LTTextBoxHorizontal(19) 139.800,399.587,457.020,409.587 'Consultant, automatisation et sécurité, PME et industrie, freelance – Paris\\n'>\n",
      "<LTTextBoxHorizontal(20) 154.800,327.587,531.992,385.631 '● Mise en place d’outils de reporting automatisé via SAP Business Objects.\\n● Traitement et sécurisation des données sensibles (data wrangling, chiffrage, redondance).\\n● Solutions de gestion de télésurveillance via objets connectés (IoT) et traitement d’images.\\n● Pentesting interne et conseil en sécurisation de réseau, mise en place de VPN.\\n● Infogérance sur serveurs et docker.\\n'>\n",
      "<LTTextBoxHorizontal(21) 40.800,303.587,90.782,313.587 '2017 – 2019\\n'>\n",
      "<LTTextBoxHorizontal(22) 139.800,303.587,428.931,313.587 'Consultant E-commerce et Marketing digital, freelance – Thaïlande\\n'>\n",
      "<LTTextBoxHorizontal(23) 154.800,279.587,559.199,289.631 '● Mission de conseils dans l’e-commerce et le marketing digital (région de Bangkok), expertise en\\n'>\n",
      "<LTTextBoxHorizontal(24) 172.800,267.587,376.262,277.587 'stratégies de vente en ligne et de marketing digital.\\n'>\n",
      "<LTTextBoxHorizontal(25) 154.800,243.587,551.834,265.631 \"● Gestion des réservations en ligne Booking.com, Airbnb, Agoda pour compte de tiers.\\n● Mise en œuvre de techniques d'optimisation des moteurs de recherche (SEO) pour améliorer la\\n\">\n",
      "<LTTextBoxHorizontal(26) 172.800,231.587,244.707,241.587 'visibilité en ligne.\\n'>\n",
      "<LTTextBoxHorizontal(27) 154.800,219.587,480.655,229.631 '● Gestion des PNL pour augmenter les réservations directes et les conversions.\\n'>\n",
      "<LTTextBoxHorizontal(28) 40.800,195.587,90.782,205.587 '2016 – 2017\\n'>\n",
      "<LTTextBoxHorizontal(29) 139.800,195.587,415.293,205.587 'Consultant web et réseaux – Australie, région du New South Wales\\n'>\n",
      "<LTTextBoxHorizontal(30) 154.800,159.587,528.644,181.631 '● Développement web dans l’e-commerce.\\n● Mise en place de réseaux d’entreprises et de solutions de virtualisation (câblage de baies,\\n'>\n",
      "<LTTextBoxHorizontal(31) 172.800,147.587,309.940,157.587 'firewalls, virtualisation proxmox).\\n'>\n",
      "<LTTextBoxHorizontal(32) 154.800,135.587,385.418,145.631 '● Gestion de firewalls et de réseaux internes (pfSense).\\n'>\n",
      "<LTTextBoxHorizontal(33) 40.800,111.587,90.782,121.587 '2013 – 2016\\n'>\n",
      "<LTTextBoxHorizontal(34) 139.800,111.695,252.995,121.195 'Gestion immobilière – Paris\\n'>\n",
      "<LTTextBoxHorizontal(35) 154.800,88.337,528.278,98.381 \"● Gestion locative d'un parc immobilier pour location touristique à Paris et supervision des\\n\">\n",
      "<LTTextBoxHorizontal(36) 172.800,76.337,268.560,86.337 'chantiers de rénovation.\\n'>\n",
      "<LTTextBoxHorizontal(37) 154.800,64.337,561.561,74.381 \"● Prospection d'appartements pour acquéreurs étrangers, conseil en investissement immobilier haut\\n\">\n",
      "<LTTextBoxHorizontal(38) 172.800,52.337,424.099,62.337 'de gamme (>1M€) et budgétisation de chantiers de rénovation.\\n'>\n",
      "<LTRect 0.000,-0.250,595.500,842.000>\n",
      "<LTLine 32.000,720.500,563.000,720.500>\n",
      "<LTLine 32.000,499.500,563.000,499.500>\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for single_page_layout in extract_pages(pdf_path):\n",
    "    counter +=1\n",
    "    print(f'Page {counter}')\n",
    "    for elt in single_page_layout:\n",
    "        print(elt)\n",
    "    if counter == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c9867-3d2f-4262-9218-5995eeb6b1af",
   "metadata": {},
   "source": [
    "# pyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d31e546-ee76-41ba-a948-1e88a30f00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import PIL.Image\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20215761-46b4-4b31-a350-6d7e99c29741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(pdf_path)\n",
    "counter = 1\n",
    "page_counter = 0\n",
    "for i in range(len(pdf)):\n",
    "    page_counter += 1\n",
    "    page = pdf[i]\n",
    "    images = page.get_images()\n",
    "    for image in images:\n",
    "        base_img = pdf.extract_image(image[0])\n",
    "        image_data = base_img[\"image\"]\n",
    "        img = PIL.Image.open(io.BytesIO(image_data))\n",
    "        extension = base_img[\"ext\"]\n",
    "        folder = f'extracted_images/page_{page_counter}'\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        img.save(open(f'{folder}/image{counter}.{extension}', \"wb\"))\n",
    "        counter += 1\n",
    "    if counter ==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60896f2a-3339-41ba-8ca6-200892be296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 0,\n",
       " 'type': 0,\n",
       " 'bbox': (237.82423400878906,\n",
       "  15.915508270263672,\n",
       "  358.20355224609375,\n",
       "  40.27878952026367),\n",
       " 'lines': [{'spans': [{'size': 22.0,\n",
       "     'flags': 20,\n",
       "     'font': 'TimesNewRomanPS-BoldMT',\n",
       "     'color': 5475540,\n",
       "     'ascender': 0.89111328125,\n",
       "     'descender': -0.21630859375,\n",
       "     'text': 'Mehdi Fekih',\n",
       "     'origin': (237.82423400878906, 35.52000045776367),\n",
       "     'bbox': (237.82423400878906,\n",
       "      15.915508270263672,\n",
       "      358.20355224609375,\n",
       "      40.27878952026367)}],\n",
       "   'wmode': 0,\n",
       "   'dir': (1.0, 0.0),\n",
       "   'bbox': (237.82423400878906,\n",
       "    15.915508270263672,\n",
       "    358.20355224609375,\n",
       "    40.27878952026367)}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'type': 0,\n",
       " 'bbox': (204.42355346679688,\n",
       "  55.657752990722656,\n",
       "  391.6080322265625,\n",
       "  82.08938598632812),\n",
       " 'lines': [{'spans': [{'size': 11.0,\n",
       "     'flags': 4,\n",
       "     'font': 'TimesNewRomanPSMT',\n",
       "     'color': 0,\n",
       "     'ascender': 0.89111328125,\n",
       "     'descender': -0.21630859375,\n",
       "     'text': '8 rue Bernard de Clairvaux, 75003 Paris',\n",
       "     'origin': (209.33460998535156, 65.45999908447266),\n",
       "     'bbox': (209.33460998535156,\n",
       "      55.657752990722656,\n",
       "      386.69403076171875,\n",
       "      67.83939361572266)}],\n",
       "   'wmode': 0,\n",
       "   'dir': (1.0, 0.0),\n",
       "   'bbox': (209.33460998535156,\n",
       "    55.657752990722656,\n",
       "    386.69403076171875,\n",
       "    67.83939361572266)},\n",
       "  {'spans': [{'size': 11.0,\n",
       "     'flags': 4,\n",
       "     'font': 'TimesNewRomanPSMT',\n",
       "     'color': 0,\n",
       "     'ascender': 0.89111328125,\n",
       "     'descender': -0.21630859375,\n",
       "     'text': '07 82 90 60 71 – mehdi.fekih@edhec.com',\n",
       "     'origin': (204.42355346679688, 79.70999145507812),\n",
       "     'bbox': (204.42355346679688,\n",
       "      69.90774536132812,\n",
       "      391.6080322265625,\n",
       "      82.08938598632812)}],\n",
       "   'wmode': 0,\n",
       "   'dir': (1.0, 0.0),\n",
       "   'bbox': (204.42355346679688,\n",
       "    69.90774536132812,\n",
       "    391.6080322265625,\n",
       "    82.08938598632812)}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "count = 0\n",
    "for i, page in enumerate(doc):\n",
    "    ext = page.get_text(\"dict\")[\"blocks\"]\n",
    "    for elt in ext:\n",
    "        if elt[\"type\"] != 1:\n",
    "            count += 1\n",
    "            if count > 2:\n",
    "                break\n",
    "            display(elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed480b2a-4c8f-431e-a59c-d7070f220a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a43e3363-ded9-4b99-ad13-e5607ea16858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigtree import shift_nodes, Node, yield_tree, findall\n",
    "from bigtree import Node, preorder_iter, postorder_iter, levelorder_iter, levelordergroup_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b2c9faa-8d40-4cf8-869f-53fa604f9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing_extensions import Self\n",
    "import os\n",
    "from os.path import basename\n",
    "import re\n",
    "import logging\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "\n",
    "# formatting-related constants for pretty-printing\n",
    "SHORT_FORMAT_SPEC = \"short\"\n",
    "FULL_FORMAT_SPEC = \"full\"\n",
    "FILTERED_METADATA_KEYS = [\"font\", \"size\", \"width\", \"height\", \"format\", \"color\"]\n",
    "\n",
    "#: label to classify nodes\n",
    "NODE_TYPE_LABEL = \"node_type\"\n",
    "NODE_TYPE_DOCUMENT = \"DOCUMENT\"\n",
    "NODE_TYPE_PAGE = \"PAGE\"\n",
    "NODE_TYPE_LINE = \"LINE\"\n",
    "NODE_TYPE_SPAN = \"SPAN\"\n",
    "NODE_TYPE_IMAGE = \"IMAGE\"\n",
    "NODE_TYPE_TABLE = \"TABLE\"\n",
    "\n",
    "class PdfContent(Node):\n",
    "    \"\"\"\n",
    "    Holds content extracted from a PDF file as a tree structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str,\n",
    "                 text: str = None,\n",
    "                 image: bytearray = None,\n",
    "                 table: DataFrame = None,\n",
    "                 metadata: dict = None,\n",
    "                 labels: dict = None,\n",
    "                 parent: Self = None):\n",
    "        \"\"\"\n",
    "        Builds a new PdfContent based on either text or an image.\n",
    "\n",
    "        :param name: (inherited) a unique label for the content node\n",
    "        :param parent: (inherited) a parent PdfContent, if not the whole PDF file itself (i.e. the tree's root)\n",
    "        :param text: some text read from the PDF\n",
    "        :param image: an image extracted from the PDF\n",
    "        :param metadata: a dict of misc metadata about the text/image source\n",
    "        :param labels: a dict of metadata about the text/image with semantic value for processing (e.g. classifiers)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(name, parent=parent, text=text, image=image, table=table, labels=labels, metadata=metadata)\n",
    "\n",
    "    def is_typed(self, node_type: str) -> bool:\n",
    "        return self.labels[NODE_TYPE_LABEL] == node_type\n",
    "\n",
    "    def __format__(self, format_spec) -> str:\n",
    "        \"\"\"\n",
    "        Print format of this PdfContent.\n",
    "        The \"full\" format_spec displays all metadata. The \"short\" one displays no metadata at all.\n",
    "\n",
    "        :return: a printable representation of the PDF\n",
    "        \"\"\"\n",
    "\n",
    "        def printable_text(n: PdfContent) -> str:\n",
    "            if n.is_typed(NODE_TYPE_LINE) or n.is_typed(NODE_TYPE_TABLE):\n",
    "                return n.text\n",
    "            if n.is_typed(NODE_TYPE_IMAGE) and format_spec == FULL_FORMAT_SPEC:\n",
    "                return to_ascii(n.image)\n",
    "            if format_spec != SHORT_FORMAT_SPEC:\n",
    "                return n.text\n",
    "            return ''\n",
    "\n",
    "        def printable_metadata(n: PdfContent) -> str:\n",
    "            if n.metadata is None or format_spec == SHORT_FORMAT_SPEC:\n",
    "                return ''\n",
    "            if format_spec == FULL_FORMAT_SPEC:\n",
    "                return \"{}\".format(n.metadata)\n",
    "            return \"{}\".format({k: v for k, v in n.metadata.items() if k in FILTERED_METADATA_KEYS})\n",
    "\n",
    "        return \"\\n\".join([\n",
    "            \"{}{}[{}] {} {}\".format(branch, stem, node.node_name, printable_text(node), printable_metadata(node))\n",
    "            for branch, stem, node in yield_tree(self, style=\"const\")\n",
    "            if format_spec != SHORT_FORMAT_SPEC or not node.is_typed(NODE_TYPE_SPAN)\n",
    "        ])\n",
    "\n",
    "    def find_nodes(self, *node_types) -> list[Self]:\n",
    "        \"\"\"\n",
    "        Returns all sub-nodes of the given type(s).\n",
    "        :return: a tuple of PdfContents\n",
    "        \"\"\"\n",
    "        return list(findall(self, condition=lambda node: node.labels[NODE_TYPE_LABEL] in node_types))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5820d7d-d8b6-419d-ba8d-c66894250296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path) -> PdfContent:\n",
    "    \"\"\"\n",
    "    Reads a source PDF file as a PdfContent.\n",
    "    The returned data structure is pre-processed: its content is as clean as possible to ease actual processing.\n",
    "\n",
    "    :param file_path: source PDF path on the file system\n",
    "    \"\"\"\n",
    "\n",
    "    with fitz.open(file_path) as doc:\n",
    "        root_node = PdfContent(name=basename(file_path),\n",
    "                               labels={NODE_TYPE_LABEL: NODE_TYPE_DOCUMENT},\n",
    "                               metadata=doc.metadata)\n",
    "        for page in doc:\n",
    "            blocks_list = page.get_text(\"dict\", sort=True)[\"blocks\"]\n",
    "            page_node = PdfContent(name=f\"page-{page.number}\",\n",
    "                                   parent=root_node,\n",
    "                                   metadata={\"page_idx\": page.number},\n",
    "                                   labels={NODE_TYPE_LABEL: NODE_TYPE_PAGE},\n",
    "                                   text=f\"{len(blocks_list)} blocks\")\n",
    "            for block_idx, block in enumerate(blocks_list):\n",
    "                if \"image\" in block:\n",
    "                    image = bytearray(block[\"image\"])\n",
    "                    image_metadata = {k: v for k, v in block.items() if k != \"image\"} | {\"block_idx\": block_idx}\n",
    "                    PdfContent(name=f\"img-{page.number}.{block_idx}\",\n",
    "                               parent=page_node,\n",
    "                               labels={NODE_TYPE_LABEL: NODE_TYPE_IMAGE},\n",
    "                               image=image,\n",
    "                               metadata=image_metadata)\n",
    "                if \"lines\" in block:\n",
    "                    for line_idx, line in enumerate(block[\"lines\"]):\n",
    "                        spans = [span[\"text\"] for span in line[\"spans\"]]\n",
    "                        line_metadata = {k: v for k, v in line.items() if k != \"spans\"} | {\"block_idx\": block_idx}\n",
    "                        line_node = PdfContent(name=f\"line-{page.number}.{block_idx}.{line_idx}\",\n",
    "                                               parent=page_node,\n",
    "                                               labels={NODE_TYPE_LABEL: NODE_TYPE_LINE},\n",
    "                                               text=''.join(spans),\n",
    "                                               metadata=line_metadata)\n",
    "                        for span_idx, span in enumerate(line[\"spans\"]):\n",
    "                            PdfContent(name=f\"span-{page.number}.{block_idx}.{line_idx}.{span_idx}\",\n",
    "                                       parent=line_node,\n",
    "                                       labels={NODE_TYPE_LABEL: NODE_TYPE_SPAN},\n",
    "                                       text=span[\"text\"],\n",
    "                                       metadata={k: v for k, v in span.items() if k != \"text\"})\n",
    "\n",
    "        sanitize(root_node, doc)\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a826884d-dc4f-4128-b602-1afdd2cebb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_empty_lines(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Modify the given PdfContent in place, removing all empty lines.\n",
    "    :param pdf: a root PdfContent\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for line in pdf.find_nodes(NODE_TYPE_LINE):\n",
    "        if re.fullmatch(\" *\", line.text):\n",
    "            shift_nodes(pdf, [line.path_name], [None])\n",
    "            count += 1\n",
    "    logging.debug(f\"removed {count} empty lines\")\n",
    "\n",
    "\n",
    "def prune_spans(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Modify the given PdfContent in place, removing all spans.\n",
    "    :param pdf: a root PdfContent\n",
    "    \"\"\"\n",
    "    for span in pdf.find_nodes(NODE_TYPE_SPAN):\n",
    "        shift_nodes(pdf, [span.path_name], [None])\n",
    "\n",
    "\n",
    "def prune_footers(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Search for footers in the given PDF and remove them in place.\n",
    "    Assume each page may finish with a same footer block, differing from at most a page number.\n",
    "    :param pdf: a multi-pages PDF document\n",
    "    \"\"\"\n",
    "    footers: list[tuple[str, list[PdfContent]]] = []  # tuples with concatenated text content & nodes\n",
    "    for page in pdf.find_nodes(NODE_TYPE_PAGE):\n",
    "        last_block_idx = max([line.metadata[\"block_idx\"] for line in page.find_nodes(NODE_TYPE_LINE)])\n",
    "        last_lines = [line for line in page.find_nodes(NODE_TYPE_LINE) if line.metadata[\"block_idx\"] == last_block_idx]\n",
    "        footers.append((\"\\t\".join([line.text for line in last_lines]), last_lines))\n",
    "\n",
    "    for footer in footers:\n",
    "        # compare with other text contents\n",
    "        diff_list = [edit_distance(footer[0], f[0]) for f in footers if f[1] != footer[1]]\n",
    "        # ignore outliers\n",
    "        max_outliers = min(len(footers) - 1, FOOTER_MAX_OUTLIERS)  # keep 1+ elements\n",
    "        for i in range(max_outliers):\n",
    "            diff_list.remove(max(diff_list))\n",
    "        # remove nodes with same content as others\n",
    "        if len(diff_list) > 0 and max(diff_list) <= FOOTER_DISTANCE_THRESHOLD:\n",
    "            logging.debug(f\"removed footer at {[f.path_name for f in footer[1]]}: {footer[0]}\")\n",
    "            shift_nodes(pdf, [f.path_name for f in footer[1]], [None] * len(footer[1]))\n",
    "def infer_line_format(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Enrich lines metadata in place, with \"font\" and \"size\" attributes, based on children spans.\n",
    "    The font is the most used one; the size is the biggest one.\n",
    "    :param pdf: some PdfContent\n",
    "    \"\"\"\n",
    "    for line in pdf.find_nodes(NODE_TYPE_LINE):\n",
    "        line_spans = line.find_nodes(NODE_TYPE_SPAN)\n",
    "        line.metadata[\"font\"] = most_used_metadata(line_spans, normalize_font)\n",
    "        line.metadata[\"size\"] = max([normalize_size(span) for span in line.find_nodes(NODE_TYPE_SPAN)])\n",
    "\n",
    "def infer_line_format(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Enrich lines metadata in place, with \"font\" and \"size\" attributes, based on children spans.\n",
    "    The font is the most used one; the size is the biggest one.\n",
    "    :param pdf: some PdfContent\n",
    "    \"\"\"\n",
    "    for line in pdf.find_nodes(NODE_TYPE_LINE):\n",
    "        line_spans = line.find_nodes(NODE_TYPE_SPAN)\n",
    "        line.metadata[\"font\"] = most_used_metadata(line_spans, normalize_font)\n",
    "        line.metadata[\"size\"] = max([normalize_size(span) for span in line.find_nodes(NODE_TYPE_SPAN)])\n",
    "\n",
    "\n",
    "def most_used_metadata(contents: Iterable[PdfContent], metadata_getter):\n",
    "    \"\"\"\n",
    "    Select one of the most used metadata in the given content.\n",
    "    :param contents: collection of contents to parse\n",
    "    :param metadata_getter: how to extract the metadata to compare\n",
    "    :return: the most used value, or None if none was found\n",
    "    \"\"\"\n",
    "    values_length: dict[int | None] = {None: 0}\n",
    "    for content in contents:\n",
    "        value = metadata_getter(content)\n",
    "        text_length = len(content.text)\n",
    "        values_length.setdefault(value, 0)\n",
    "        values_length[value] += text_length\n",
    "    main_font_length = max(values_length.values())\n",
    "    return [k for k, v in values_length.items() if v == main_font_length][0]\n",
    "\n",
    "\n",
    "def merge_similar_sibling_lines(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Modifies the given PdfContent in place,\n",
    "    merging sibling lines when coming from the same block or having similar format.\n",
    "    :param pdf: a PdfContent with content to merge\n",
    "    \"\"\"\n",
    "    all_pages = list(pdf.find_nodes(NODE_TYPE_PAGE))\n",
    "    for page in all_pages:\n",
    "        # create a new page container\n",
    "        new_page = PdfContent(name=f\"{page.node_name}-merged\",\n",
    "                              parent=pdf,\n",
    "                              labels={NODE_TYPE_LABEL: NODE_TYPE_PAGE},\n",
    "                              metadata=page.metadata)\n",
    "\n",
    "        # mark mergeable lines (to the right)\n",
    "        mergeable_lines = [\n",
    "            child.is_typed(NODE_TYPE_LINE) and child.right_sibling is not None\n",
    "            and child.right_sibling.is_typed(NODE_TYPE_LINE) and same_format(child, child.right_sibling)\n",
    "            for child in page.children\n",
    "        ]\n",
    "\n",
    "        i = -1\n",
    "        j = 0\n",
    "        while j < len(mergeable_lines):\n",
    "            if not mergeable_lines[j]:  # end of mergeable sub-list\n",
    "                if i < j - 1:\n",
    "                    source_lines = page.children[0:j - i]\n",
    "                    # create an aggregating line in the new page\n",
    "                    new_line = PdfContent(name=f\"{source_lines[0].node_name}-{source_lines[-1].node_name}\",\n",
    "                                          parent=new_page,\n",
    "                                          text=re.sub(\" +\", \" \", \" \".join([line.text for line in source_lines])),\n",
    "                                          labels={NODE_TYPE_LABEL: NODE_TYPE_LINE},\n",
    "                                          metadata={\n",
    "                                              \"sources\": {s.node_name: s.metadata for s in source_lines},\n",
    "                                              \"font\": normalize_font(source_lines[0]),\n",
    "                                              \"size\": normalize_size(source_lines[0]),\n",
    "                                          })\n",
    "                    # migrate children and remove merged lines\n",
    "                    for source_line in source_lines:\n",
    "                        for child in source_line.children:\n",
    "                            shift_nodes(pdf, [child.path_name], [f\"{new_line.path_name}/{child.node_name}\"])\n",
    "                        shift_nodes(pdf, [source_line.path_name], [None])\n",
    "                else:\n",
    "                    unchanged_line = page.children[0]\n",
    "                    shift_nodes(pdf, [unchanged_line.path_name], [f\"{new_page.path_name}/{unchanged_line.node_name}\"])\n",
    "                i = j\n",
    "            j += 1\n",
    "\n",
    "        # cleanup\n",
    "        shift_nodes(pdf, [page.path_name], [None])\n",
    "\n",
    "\n",
    "def infer_titles(pdf: PdfContent):\n",
    "    \"\"\"\n",
    "    Modifies the given PdfContent in place, introducing a line hierarchy based on font size.\n",
    "    Images are always considered a direct subsection of their predecessor node.\n",
    "    :param pdf: a whole PdfContent\n",
    "    \"\"\"\n",
    "\n",
    "    # no hierarchy below the main font size\n",
    "    main_font_size = most_used_metadata(pdf.find_nodes(NODE_TYPE_LINE), normalize_size)\n",
    "\n",
    "    for image in pdf.find_nodes(NODE_TYPE_IMAGE):\n",
    "        if image.left_sibling is not None and image.left_sibling.is_typed(NODE_TYPE_LINE):\n",
    "            shift_nodes(pdf, [image.path_name], [f\"{image.left_sibling.path_name}/{image.node_name}\"])\n",
    "\n",
    "    all_lines = [line for line in pdf.find_nodes(NODE_TYPE_LINE, NODE_TYPE_TABLE)]\n",
    "    for line in all_lines:\n",
    "        while is_subsection(line, main_font_size):\n",
    "            shift_nodes(pdf, [line.path_name], [f\"{line.left_sibling.path_name}/{line.node_name}\"])\n",
    "\n",
    "\n",
    "def is_subsection(line: PdfContent, main_font_size: float) -> bool:\n",
    "    \"\"\"\n",
    "    Tells if the given line is a subsection of its left sibling.\n",
    "    :param line: a line\n",
    "    :param main_font_size: the font size titles should exceed\n",
    "    :return: if the given line should be a subsection of its left sibling\n",
    "    \"\"\"\n",
    "    sibling = line.left_sibling\n",
    "    if sibling is None or not sibling.is_typed(NODE_TYPE_LINE) or normalize_size(sibling) <= main_font_size:\n",
    "        return False\n",
    "    return normalize_size(line) < normalize_size(sibling)\n",
    "\n",
    "\n",
    "def normalize_size(content: PdfContent) -> (float, None):\n",
    "    \"\"\"\n",
    "    Normalize the given node's size in order to compare values more leniently.\n",
    "    :param content: some PdfContent\n",
    "    :return: a comparable, rounded size; or None if not applicable\n",
    "    \"\"\"\n",
    "    raw_size = content.metadata[\"size\"] if \"size\" in content.metadata else None\n",
    "    if raw_size is None:\n",
    "        return None\n",
    "    return round(float(raw_size) * 4, 0) / 4  # rounded to 0.25\n",
    "\n",
    "\n",
    "def normalize_font(content: PdfContent) -> (str, None):\n",
    "    \"\"\"\n",
    "    Normalize the given node's font name in order to compare values more leniently.\n",
    "    :param content: some PdfContent\n",
    "    :return: a simpler, comparable font name; or None if not applicable\n",
    "    \"\"\"\n",
    "    raw_font = content.metadata[\"font\"] if \"font\" in content.metadata else None\n",
    "    if raw_font is None:\n",
    "        return None\n",
    "    return re.sub(\"-(Bold|Italic|Black|Regular)+|MT\\\\b|PS\\\\b\", \"\", raw_font)  # ignore modifiers & suffixes\n",
    "\n",
    "\n",
    "def same_format(content1: PdfContent, content2: PdfContent) -> bool:\n",
    "    \"\"\"\n",
    "    Returns if the given PdfContents have similar text formatting.\n",
    "    :param content1: some content\n",
    "    :param content2: another content\n",
    "    :return: if the given contents are similar, formatting-wise; or None if not comparable\n",
    "    \"\"\"\n",
    "    font1 = normalize_font(content1)\n",
    "    font2 = normalize_font(content2)\n",
    "    size1 = normalize_size(content1)\n",
    "    size2 = normalize_size(content2)\n",
    "    if font1 is None or font2 is None or size1 is None or size2 is None:\n",
    "        return False\n",
    "    same_font = font1 == font2\n",
    "    same_font_size = abs(size1 - size2) < .5\n",
    "    return same_font and same_font_size\n",
    "\n",
    "\n",
    "def to_ascii(image: bytearray) -> str:\n",
    "    \"\"\"\n",
    "    Provides a textual, terminal-compatible view of the given image.\n",
    "    For debug purposes only.\n",
    "\n",
    "    :param image: any image\n",
    "    :return: a colored ASCII-art image\n",
    "    \"\"\"\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(prefix=\"unibsv-\", delete=False)\n",
    "    with temp_file:\n",
    "        temp_file.write(image)\n",
    "    ascii_art = AsciiArt.from_image(temp_file.name).to_ascii(columns=100)\n",
    "    os.unlink(temp_file.name)\n",
    "    return \"\\n\" + ascii_art\n",
    "\n",
    "def merge_all_pages(pdf) -> PdfContent:\n",
    "    \"\"\"\n",
    "    Modifies the given PdfContent in place, merging all pages into a new one.\n",
    "    :param pdf: a root PdfContent with pages\n",
    "    :return: the new page with all content\n",
    "    \"\"\"\n",
    "    all_pages = [page for page in pdf.find_nodes(NODE_TYPE_PAGE)]\n",
    "\n",
    "    # add a new page as container\n",
    "    single_page = PdfContent(name=\"single-page\",\n",
    "                             parent=pdf,\n",
    "                             labels={NODE_TYPE_LABEL: NODE_TYPE_PAGE},\n",
    "                             metadata={\"sources\": {page.node_name: page.metadata for page in all_pages}})\n",
    "\n",
    "    # move all children\n",
    "    for page in all_pages:\n",
    "        for from_line in page.children:\n",
    "            shift_nodes(pdf, [from_line.path_name], [f\"{single_page.path_name}/{from_line.node_name}\"])\n",
    "\n",
    "    # cleanup\n",
    "    for page in all_pages:\n",
    "        shift_nodes(pdf, [page.path_name], [None])\n",
    "\n",
    "    logging.debug(f\"merged {len(all_pages)} pages\")\n",
    "    return single_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d26d2a2-52b7-46b8-a0da-c1ced3f90451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(pdf: PdfContent, doc: fitz.Document):\n",
    "    \"\"\"\n",
    "    Process in place the given PdfContent to ensure it's in a convenient format for further processing.\n",
    "    * Based on font/size metadata, aggregate spans and lines into paragraphs\n",
    "    * Include tables whenever possible\n",
    "    * Remove useless lines whenever possible (e.g. footers)\n",
    "    * Infer a title/section hierarchy based on font size and line order\n",
    "    * Simplify the tree structure (single page and no spans)\n",
    "    :param pdf: a raw PdfContent\n",
    "    :param doc: the source document (must not be closed yet, to enable extracting additional content)\n",
    "    \"\"\"\n",
    "\n",
    "    prune_empty_lines(pdf)\n",
    "    infer_line_format(pdf)\n",
    "    prune_spans(pdf)  # at this point spans are not useful anymore\n",
    "    #prune_footers(pdf)\n",
    "    #substitute_tables(pdf, doc)\n",
    "    merge_all_pages(pdf)\n",
    "    #merge_similar_sibling_lines(pdf)\n",
    "    infer_titles(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4395723-334a-4ce8-a671-28bf5c33376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = read_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b055013-d0b9-4ec7-a498-428b8ef93607",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in preorder_iter(pdf):\n",
    "    break\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dbc572-a6b1-4b60-b4b2-6b5887ae4f42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "937efe45-d4e8-44ce-a08d-bd146009a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.11.0\n",
      "TensorFlow-IO Version:  0.32.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "print(\"TensorFlow Version: \", tf.__version__)\n",
    "print(\"TensorFlow-IO Version: \", tfio.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aea27853-2fe8-4faa-8e6c-10e49bafd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eec27a46-e18a-4e30-a662-5469683fbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open(pdf_path)\n",
    "text = doc.load_page(0).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f984c974-2a86-4b14-a9dc-3da31669aab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'FloatTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp_sm \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfr_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:442\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:478\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/fr_core_news_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:659\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:516\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    514\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n\u001b[1;32m    515\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[0;32m--> 516\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/util.py:564\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[1;32m    563\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 564\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mlang_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/language.py:1773\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1767\u001b[0m warn_if_jupyter_cupy()\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;66;03m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;66;03m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# and then again when we load from disk.\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mlang_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after_creation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1775\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m after_creation(nlp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy/language.py:162\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[0;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialise a Language object.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mvocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/language#init\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# We're only calling this to import all factories provided via entry\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# points. The factory decorator applied to these functions takes care\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# of the rest.\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entry_point_factories\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m DEFAULT_CONFIG\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_config)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(meta)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/catalogue/__init__.py:119\u001b[0m, in \u001b[0;36mRegistry.get_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_points:\n\u001b[0;32m--> 119\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys, value \u001b[38;5;129;01min\u001b[39;00m REGISTRY\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace[i] \u001b[38;5;241m==\u001b[39m keys[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace))\n\u001b[1;32m    123\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/catalogue/__init__.py:134\u001b[0m, in \u001b[0;36mRegistry.get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry_point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_entry_points():\n\u001b[0;32m--> 134\u001b[0m     result[entry_point\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mentry_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/importlib/metadata/__init__.py:171\u001b[0m, in \u001b[0;36mEntryPoint.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the entry point from its definition. If only a module\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mis indicated by the value, return that module. Otherwise,\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mreturn the named object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 171\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodule\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mgetattr\u001b[39m, attrs, module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m architectures\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotation_setters\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m span_getters\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/architectures.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ragged, Floats2d\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerModel, TransformerListener\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trfs2arrays, split_trf_batch\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/layers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlistener\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerListener\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_trf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_trf_batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/layers/listener.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerData\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerListener\u001b[39;00m(Model):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A layer that gets fed its answers from an upstream connection,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    for instance from a component earlier in the pipeline.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/spacy_transformers/data_classes.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutput\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ragged, Floats2d, Floats3d, FloatsXd, Ints1d, Ints2d\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps, get_array_module, xp2torch, torch2xp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/modeling_outputs.py:25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseModelOutput\u001b[39;00m(ModelOutput):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Base class for model's outputs, with potential hidden states and attentions.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m            heads.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     last_hidden_state: torch\u001b[38;5;241m.\u001b[39mFloatTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/modeling_outputs.py:45\u001b[0m, in \u001b[0;36mBaseModelOutput\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseModelOutput\u001b[39;00m(ModelOutput):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Base class for model's outputs, with potential hidden states and attentions.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m            heads.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     last_hidden_state: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     attentions: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'FloatTensor'"
     ]
    }
   ],
   "source": [
    "nlp_sm = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91a6ec-192f-4cb9-8088-30c7230a6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_entities(pipeline, text):\n",
    "    \n",
    "    # Create a document \n",
    "    document = pipeline(text)\n",
    "    \n",
    "    # Entity text & label extraction\n",
    "    for entity in document.ents:\n",
    "        print(entity.text + '->', entity.label_)\n",
    "        \n",
    "        \n",
    "def visualize_entities(pipeline, text):\n",
    "    \n",
    "    # Create a document \n",
    "    document = pipeline(text)\n",
    "        \n",
    "    # Show entities in pretty manner\n",
    "    displacy.render(document, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff9693-bf1b-4acf-b139-1a9eed7b752a",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570b847-13c3-43da-a68f-0c86e10f613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import shutil\n",
    "\n",
    "UPLOAD_DIR = \"uploads\"  # Directory to save the uploaded files\n",
    "\n",
    "# Create the upload directory if it doesn't exist\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def upload_file(file):\n",
    "    file_path = os.path.join(UPLOAD_DIR, file.name)\n",
    "    shutil.move(file_path, UPLOAD_DIR)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    file_output = gr.File()\n",
    "    upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\"pdf\", \"doc\"], file_count=\"single\")\n",
    "    upload_button.upload(upload_file, upload_button, file_output)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106b4ac-1d5c-4e4d-958d-8c3d07c2630d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da957f-f5a6-4786-8264-99c33ce664cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume",
   "language": "python",
   "name": "resume"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
